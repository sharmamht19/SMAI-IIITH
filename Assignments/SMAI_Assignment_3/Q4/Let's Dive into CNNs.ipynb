{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80485b1d",
   "metadata": {},
   "source": [
    "# Let's Dive into CNNs\n",
    "## Please read the following paragraph I copy-pasted from ChatGPT:\n",
    "Welcome back to the exciting world of Convolutional Neural Networks (CNN), building on our previous question which focused on Multilayer Perceptrons (MLP)! As you already know, CNNs are a powerful tool for image and video processing applications. They are designed to recognize patterns and features in visual data, making them a popular choice in computer vision research. One of the reasons why CNNs are preferred over MLPs for image recognition tasks is their ability to handle high-dimensional input data, which preserves spatial information by applying convolutional filters to detect patterns and features at different locations in the input data.\n",
    "\n",
    "One of the reasons why CNNs are preferred over MLPs for image recognition tasks is their ability to handle high-dimensional input data. MLPs are limited in their ability to process images because they require that the input data be flattened into a one-dimensional vector, which can result in a loss of important spatial information. In contrast, CNNs are designed to handle multi-dimensional data such as images, preserving spatial information by applying convolutional filters to detect patterns and features at different locations in the input data.\n",
    "\n",
    "In this assignment, we will be playing with the CIFAR-10 dataset to explore different CNN models. We will start by training an auto-encoder for CIFAR images. This auto-encoder will allow us to compare different models, including a Multilayer Perceptron (MLP) model, a CNN-MLP mixed model, and finally a CNN-only model. By comparing the performance of these models, we can gain a deeper understanding of the importance of CNN layers in image processing tasks.\n",
    "\n",
    "Once we've completed our auto-encoder experiments, we will use the encoder from the best-performing model to train a Siamese network. Siamese networks are a type of neural network that can recognize similarity between two images, making them a popular choice for tasks such as image retrieval and face recognition. By training a Siamese network using the CIFAR-10 dataset, we can gain a better understanding of how CNNs can be used in more complex image recognition tasks.\n",
    "\n",
    "Overall, this assignment will allow us to explore the many different applications of CNNs in image processing tasks. We'll have the opportunity to experiment with different architectures, compare their performance, and gain valuable insights into how CNNs can be used to solve real-world problems.\n",
    "\n",
    "ðŸ«µPro-tip: Utilize learnings from previous question to better solve problems in this question.\n",
    "\n",
    "ðŸ’­Trivia: Yes, I am allowed use ChatGPT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528fa70",
   "metadata": {},
   "source": [
    "## Part 0: Initialization\n",
    "### Step zero: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b3be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cb7a6",
   "metadata": {},
   "source": [
    "### Step one: Using a PyTorch Dataset\n",
    "Load CIFAR-10 dataset from `torchvision.datasets`. Keep batch_size >= 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the dataset\n",
    "def imshow(img):\n",
    "    img = img #/ 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea766ad0",
   "metadata": {},
   "source": [
    "## Part 1: Auto-Encoder\n",
    "### Step one: Define 3 Models for Auto-Encoder\n",
    "1. MLP-only model\n",
    "2. CNN-MLP combination model\n",
    "3. CNN-only model\n",
    "\n",
    "In all the models, encoder and decoder should be consisting of $3$ layers each, and the encoder should be giving a flattened representation of size $32$.\n",
    "\n",
    "Please see that each layer of encoder should decrease or keep equal the output size as compared to input size. Similarly, each layer of decoder should increase or keep equal the output size as compared to input size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAutoEncoder(nn.Module):\n",
    "    # code\n",
    "class ComboAutoEncoder(nn.Module):\n",
    "    # code\n",
    "class CNNAutoEncoder(nn.Module):\n",
    "    # code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code for a autoencoder:\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        # Representation size: 3072\n",
    "        # Number of Layers for encoder: 1\n",
    "        # Number of Layers for decoder: 1\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1), # 3x32x32 -> 12x16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), # 12x16x16 -> 3072\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (12, 16, 16)), # 3072 -> 12x16x16\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1), # 12x16x16 -> 3x32x32\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816bd2de",
   "metadata": {},
   "source": [
    "### Step two: Using MSELoss, train all the models and analyse results\n",
    "Make sure you also discuss size, time-taken and any other differences you found. Plot MSELoss for each epoch for all models. Also, tabulate the final MSELosses of all models and plot it vis-a-vis:\n",
    "\n",
    "1. Model Size\n",
    "2. Model Parameters\n",
    "3. Time Taken per epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f68403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1468ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize the following code\n",
    "#--------------MLP--------------#\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "images = Variable(images)\n",
    "\n",
    "print(\"Reconstructed image\")\n",
    "decoded_imgs = autoencoder(images)[1]\n",
    "imshow(torchvision.utils.make_grid(decoded_imgs.data))\n",
    "\n",
    "#--------------Combo--------------#\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "images = Variable(images)\n",
    "\n",
    "print(\"Reconstructed image\")\n",
    "decoded_imgs = autoencoder(images)[1]\n",
    "imshow(torchvision.utils.make_grid(decoded_imgs.data))\n",
    "\n",
    "#--------------CNN--------------#\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "images = Variable(images)\n",
    "\n",
    "print(\"Reconstructed image\")\n",
    "decoded_imgs = autoencoder(images)[1]\n",
    "imshow(torchvision.utils.make_grid(decoded_imgs.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis, plots and tabulations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83832e4f",
   "metadata": {},
   "source": [
    "### Step three: Let's play with Representation Sizes\n",
    "Spoilers: You must have found CNN to be more effective.\n",
    "\n",
    "Have you ever wondered how varying the size of a CNN auto-encoder's representation can affect its performance? If so, here's a challenge for you: Implement a CNN auto-encoder with different representation sizes and observe the impact on the reconstruction quality.\n",
    "\n",
    "Play with the following representation sizes:\n",
    "1. 10\n",
    "2. 32\n",
    "3. 100\n",
    "4. 1000\n",
    "\n",
    "When working with different representation sizes in a CNN auto-encoder, we may reach a point of diminishing returns where increasing the representation size does not lead to significant improvements in reconstruction quality. This can be due to factors such as increased model complexity and longer training times. Discuss where you feel we might reach this point of diminishing returns when working with CIFAR-10 dataset and varying representation sizes.\n",
    "\n",
    "Do analysis as suggested earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98e333",
   "metadata": {},
   "source": [
    "### Step four: What about the number of layers?\n",
    "\n",
    "Experimenting with different numbers of layers in the encoder and decoder of a CNN auto-encoder can have a significant impact on its performance with CIFAR-10 dataset. For our next challenge, let's explore how the number of layers can affect the quality of the reconstructed images. Would a deeper or shallower network be more effective for this task?\n",
    "\n",
    "Play with the following numbers of layers for encoder and decoder each:\n",
    "1. 1\n",
    "2. 3\n",
    "3. 5\n",
    "4. 10\n",
    "\n",
    "When experimenting with different numbers of layers in the encoder and decoder of a CNN auto-encoder, we may reach a point of diminishing returns where adding more layers does not result in significant improvements in reconstruction quality. Additionally, deeper networks may be prone to overfitting, especially when training data is limited. Discuss where you feel we reach it. \n",
    "\n",
    "Do analysis as suggested earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd99353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42eccfa",
   "metadata": {},
   "source": [
    "### Explaination time!\n",
    "Please write a short-essay explaining what AutoEncoder does, how it is different from other data-representation methods like PCA, t-SNE and Variational AutoEncoders. Also tell what were your take-outs from this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainations here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa267490",
   "metadata": {},
   "source": [
    "## Part 2: The return of kNN\n",
    "Autoencoder representations can be useful for a wide range of machine learning tasks beyond just reconstructing input data. For example, the output of an encoder can be used as a feature representation for classification using k-Nearest Neighbors (kNN) or other classifiers. Similarly, the encoded representations can be used for unsupervised tasks like clustering, where the model groups similar images together based on their feature representation. This makes autoencoders a versatile tool for feature learning, with the potential to enhance the performance of a wide range of machine learning algorithms.\n",
    "\n",
    "Since we have already trained auto-encoder on CIFAR-10, we will employ the encoder from the CNN model with representation size of $10$. Using a subset of CIFAR-10 train and test datasets, each with 10,000 and 1000 samples respectively, with an equal number of samples for each class, implement kNN classification using the encoder output as features. Share your findings and accuracy, F1 score and other relavant metrics on the test set. What insights can you draw from the results?\n",
    "\n",
    "You are allowed to use sklearn or any other library for kNN and splitting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and analysis here. Remember to keep your analysis of high quality at all times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a15ce8",
   "metadata": {},
   "source": [
    "## Part 3: Siamese Network (not part of assignment)\n",
    "Siamese networks are useful for comparing and finding similarities between two inputs. The inputs from identical encoder sub-networks will be given as an input to a dense network (an MLP) and the prediction will be given as output of a Sigmoid function.\n",
    "\n",
    "Employ the encoder from the best model to be used as the two-identical sub-networks of the Siamese Network. So, copy the weights from the best encoder to the two-identical sub-networks, set the learning rate of this encoder to be lower by a couple of order than that of the dense network.\n",
    "\n",
    "Who knows, this might be part of next assignment during hectic times. (wink wink)\n",
    "(Don't quote me on this though)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a12b4",
   "metadata": {},
   "source": [
    "### Step one: Define a Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample.\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder1 = encoder\n",
    "        self.encoder2 = encoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2048,1024),\n",
    "            # whatever\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        encoded1 = self.encoder1(x1)\n",
    "        encoded2 = self.encoder2(x2)\n",
    "        return (encoded1,encoded2)\n",
    "        \n",
    "    def differentiate(self, x1_embedding, x2_embedding):\n",
    "        x = torch.cat([x1_embedding,x2_embedding],dim=1)\n",
    "        return self.decoder(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3c5ee",
   "metadata": {},
   "source": [
    "### Step two: Define the following three losses:\n",
    "1. TripletLoss\n",
    "2. ContrastiveLoss\n",
    "3. Regularized Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TripletLoss defined as an example\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Compute the distances between the anchor, positive, and negative examples\n",
    "        distance_pos = F.pairwise_distance(anchor, positive, p=2)\n",
    "        distance_neg = F.pairwise_distance(anchor, negative, p=2)\n",
    "        # Compute the triplet loss using the margin\n",
    "        loss_triplet = torch.mean(torch.clamp(distance_pos - distance_neg + self.margin, min=0.0))\n",
    "        return loss_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d748e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1409957",
   "metadata": {},
   "source": [
    "### Train the Siamese network...\n",
    "... and write a function that takes 2 inputs and outputs whether they belong to the same class or not. Plot and share the results of the function. The results should contain both successful results, i.e. classification as 'same' for inputs belonging to the same class and classification as 'not-same' for inputs belonging to different classes, as well as some unsuccessful results (that gives the wrong output than expected).\n",
    "Provide an explanation of why the unsuccessful results might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d58fa4",
   "metadata": {},
   "source": [
    "### And again. What did we learn?\n",
    "Discuss any and all learnings here. The discussions must be all-encompassing so that we know what did you learn. \n",
    "\n",
    "To re-iterate: \"Please do not copy from your friend or copy-paste from the internet. We can see repetitions during evaluations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f834a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
