{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a736d316-20aa-4263-84c3-8afc5c4a9cc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# SMAI Assignment - 3\n",
    "\n",
    "## Question - `3` : Knowing MLPs\n",
    "\n",
    "| | |\n",
    "|- | -|\n",
    "| Course | Statistical Methods in AI |\n",
    "| Release Date | `09.03.2023` |\n",
    "| Due Date | `24.03.2023` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86307a-7690-40cf-9eb2-e29fab7b15b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Please read the following before moving on\n",
    "\n",
    "Welcome to the world of Multilayer Perceptrons (MLP)! You already know that MLPs are feedforward neural networks consisting of multiple layers of nodes or neurons. These networks are well-suited for a wide range of classification and regression tasks, thanks to their ability to learn complex, non-linear relationships between inputs and outputs.\n",
    "\n",
    "In this assignment, we will be working with the MNIST dataset to explore the importance of different MLP components. The MNIST dataset consists of 70,000 handwritten digit images, each of which is 28x28 pixels in size. Our goal is to use an MLP to classify these images into one of 10 categories (0-9).\n",
    "\n",
    "To improve the performance of our MLP, we will experiment with various techniques such as Dropout, Batch Normalization, Loss Functions, Stochastic batch and mini-batch gradient descent, and more. Please note, you must use mini-batch unless explicity specified.\n",
    "\n",
    "In addition, we will experiment with different optimization algorithms such as stochastic gradient descent, Adam, and RMSprop to find the optimal weights and biases for our MLP. We will use stochastic batch and mini-batch gradient descent, which involve updating the weights and biases of the network based on a small batch of randomly sampled training examples, to speed up the training process and reduce memory usage.\n",
    "\n",
    "By the end of this assignment, you will have gained a deeper understanding of the various components that make up an MLP and their importance in achieving high performance in classification tasks. You will have gained hands-on experience in experimenting with these components and learned how to fine-tune an MLP to achieve the best possible performance on the MNIST dataset. So, let's get started!\n",
    "\n",
    "ü´µPro-tip: Do not re-write any results so as to re-use them in later experiments for tabulation and plotting.\n",
    "\n",
    "üí≠Trivia: Did you know code written using ChatGPT is easy to catch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f25c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step zero: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0546f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step one: Using a PyTorch Dataset\n",
    "Load MNIST dataset from `torchvision.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                    transforms.ToTensor(), \n",
    "            ])\n",
    "\n",
    "trainset = #TODO use datasets.MNIST\n",
    "testset = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bf20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader_minibatch = torch.utils.data.DataLoader(trainset, batch_size=64\n",
    "                                                    , shuffle=True, num_workers=2)\n",
    "trainloader_stochastic = torch.utils.data.DataLoader(trainset, batch_size=1\n",
    "                                                     , shuffle=True, num_workers=2)\n",
    "testloader = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179becdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=len(trainloader_minibatch))\n",
    "for idx, (data,label) in enumerate(trainloader_minibatch):\n",
    "    print(idx,data.size(),label.size())\n",
    "    pbar.update(1)\n",
    "    break\n",
    "pbar.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25020e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step two: Define a MLP Model and without any bells and whitsles...\n",
    "... along with a CrossEntropy loss criterion \n",
    "\n",
    "Do not use Dropout, BN or any other thing. Use ReLU for hidden layers.\n",
    "\n",
    "‚ö†Ô∏è Do not use SoftMax in the output as nn.CrossEntropyLoss combines SoftMax and NLLLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49dc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP,self).__init__()\n",
    "        # code here\n",
    "    def forward(self,x):\n",
    "        # code here\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e81eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step three: Define the following optimizers using nn.optim\n",
    "1. SGD\n",
    "2. SGD with momentum\n",
    "3. SGD with L2 regularization\n",
    "4. RMSprop\n",
    "5. Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fa204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0f6b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step four: Run the SimpleMLP using different optimizers and plot train and test loss for each optimizer.\n",
    "Explain the results.\n",
    "\n",
    "Report final accuracy, F1 score and other relavant metrics in a tabular form on test and train datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code, plots and explainiation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d772c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code: need not rely on this\n",
    "EPOCHS = 25\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(total=len(trainloader))\n",
    "    out_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        # do things\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "        \n",
    "        out = mlp(data)\n",
    "        # do things\n",
    "        out_loss += loss.cpu().data.item()\n",
    "        # do things\n",
    "        pbar.update(1)\n",
    "        pbar.desc= f'Loss: {loss.item()}'\n",
    "    train_loss.append(out_loss/len(trainloader))\n",
    "    with torch.no_grad():\n",
    "        out_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(testloader):\n",
    "            # do things\n",
    "            \n",
    "        val_loss.append(out_loss/len(testloader))\n",
    "    print()\n",
    "    pbar.refresh()\n",
    "    pbar.close()\n",
    "\n",
    "plot_losses(train_loss,val_loss)\n",
    "final_metrics = get_metrics_somehow(mlp,trainloader,testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ffca0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step five: Using SimpleMLP and Adam optimizer, train models using 2 different lr_schedulers.\n",
    "Select 2 of **MultiplicativeLR**, **MultiStepLR**, **LinearLR** and **ExponentialLR**\n",
    "\n",
    "Compare the results among different LR schedulers and the original model which didn't employ any LR scheduler. Compile results in a tabular form. Plot losses for each. Explain results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e61fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91631d83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step six: Define 3 models with following changes:\n",
    "1. Add BatchNorm\n",
    "2. Add Dropout\n",
    "3. Add BatchNorm and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d845a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc3d78f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step seven: Train the above models and compare with SimpleMLP.\n",
    "\n",
    "Use your choice of optimizer, use no lr_scheduler so as to re-use the previous results.\n",
    "\n",
    "Perform analysis. You've got the drill by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e3334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step eight: Mini-batch vs Stocastic\n",
    "Now that you might have a clear winner in your mind regarding which model and settings perform the best, train it on mini-batch and stocastic and compare time taken, loss curve, accuracy etc.\n",
    "\n",
    "Perform an analysis like never before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f04f42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### And most importantly!!! What did we learn?\n",
    "Discuss any and all learnings here. The discussions must be all-encompassing so that we know what did you learn. \n",
    "\n",
    "Please do not copy from your friend or copy-paste from the internet. We can see repetitions during evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
